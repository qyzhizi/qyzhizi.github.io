<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.112.5"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="转置卷积与卷积的理解"><meta itemprop=description content="转置卷积与卷积的理解"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/2001_big_icons.png"><meta itemprop=keywords content="转置卷积,卷积,programming"><meta property="og:type" content="article"><meta property="og:title" content="转置卷积与卷积的理解"><meta property="og:description" content="转置卷积与卷积的理解"><meta property="og:image" content="/imgs/2001_big_icons.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/trans-conv-and-conv.html"><meta property="og:site_name" content="log"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Hollis"><meta property="article:published_time" content="2023-06-20 08:09:46 +0800 CST"><meta property="article:modified_time" content="2023-06-20 08:09:46 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.8b63813f6d971d6efc075de818d178312c142a542313b8ac1b773085bbc915ef.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"math":{"js":{"file":"es5/tex-mml-chtml.js","name":"mathjax","version":"3.2.0"},"render":"mathjax"},"path":"trans-conv-and-conv.html","permalink":"/post/trans-conv-and-conv.html","title":"转置卷积与卷积的理解","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><title>转置卷积与卷积的理解 - log</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>log</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>programming</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-flinks"><a href=/flinks.html class=hvr-icon-pulse rel=section><i class="fa fa-thumbs-up hvr-icon"></i>站点示例</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>11</span></a></li><li class="menu-item menu-item-commonweal"><a href=/404.html class=hvr-icon-pulse rel=section><i class="fa fa-heartbeat hvr-icon"></i>公益 404</a></li><li class="menu-item menu-item-RSS"><a href=/rss.xml class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>RSS</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#转置卷积含义与矩阵形式>转置卷积含义与矩阵形式</a></li><li><a href=#转置卷积的一种简单理解>转置卷积的一种简单理解</a></li><li><a href=#pytorch-转置卷积参数的理解及其shape的公式推导>pytorch 转置卷积参数的理解及其Shape的公式推导</a><ul><li><a href=#pytorch-转置卷积shape的计算公式>pytorch 转置卷积shape的计算公式</a></li><li><a href=#pytorch-转置卷积-的计算过程>pytorch 转置卷积 的计算过程</a><ul><li><a href=#第一步对输入的特征图进行一些变换>第一步，对输入的特征图进行一些变换</a></li><li><a href=#第二步对变换得到特征图进行普通的卷积>第二步，对变换得到特征图进行普通的卷积</a></li></ul></li></ul></li><li><a href=#卷积与数学上的卷积>卷积与数学上的卷积</a><ul><li><a href=#pytorch-的转置卷积的核旋转了180度>pytorch 的转置卷积的核旋转了180度</a></li><li><a href=#数学上定义的卷积>数学上定义的卷积</a></li><li><a href=#多项式系数卷积与意义>多项式系数卷积与意义</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Hollis src=/imgs/img-lazy-loading.gif data-src=/imgs/2001_big_icons.png><p class=site-author-name itemprop=name>Hollis</p><div class=site-description itemprop=description>share something</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>11</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>2</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>25</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/qyzhizi title="Github → https://github.com/qyzhizi" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://twitter.com/qyzhizi title="Twitter → https://twitter.com/qyzhizi" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-twitter fa-fw hvr-icon"></i>Twitter</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-06-03T11:52:18+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=15267></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=38></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-06-20T08:09:46+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=goto-gtranslate class=button title=多语言翻译><i class="fas fa-globe"></i></div><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><a role=button class="book-mark-link book-mark-link-fixed"></a>
<a href=https://github.com/qyzhizi rel="noopener external nofollow noreferrer" target=_blank title="Follow me on GitHub" class="exturl github-corner"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><script type=text/javascript src=//sidecar.gitter.im/dist/sidecar.v1.js async></script>
<script type=text/javascript>((window.gitter={}).chat={}).options={room:"hugo-next/community"}</script><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/trans-conv-and-conv.html><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/2001_big_icons.png"><meta itemprop=name content="Hollis"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Hollis"><meta itemprop=description content="share something"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="转置卷积与卷积的理解"><meta itemprop=description content="转置卷积与卷积的理解"></span><header class=post-header><h1 class=post-title itemprop="name headline">转置卷积与卷积的理解
<a href=https://github.com/qyzhizi/hugo-blog/tree/main/content/post/%e8%bd%ac%e7%bd%ae%e5%8d%b7%e7%a7%af%e4%b8%8e%e5%8d%b7%e7%a7%af%e7%9a%84%e7%90%86%e8%a7%a3.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2023-06-20 08:09:46 +0800 CST" itemprop="dateCreated datePublished" datetime="2023-06-20 08:09:46 +0800 CST">2023-06-20</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/programming itemprop=url rel=index><span itemprop=name>programming</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>7420</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>15分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span class=waline-pageview-count data-path=/post/trans-conv-and-conv.html><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p>转置卷积与卷积的理解，本文将介绍：</p><ul><li>转置卷积含义与矩阵形式</li><li>转置卷积的一种简单理解</li><li>pytorch 转置卷积参数的理解及其Shape的公式推导</li><li>卷积与数学上的卷积，卷积核旋转180度</li></ul><p>阅读前提：</p><ul><li>理解深度学习中普通卷积的概念与shape的计算公式</li><li>了解深度学习框架pytorch卷积api的调用</li><li>了解卷积的矩阵运算形式</li></ul><h2 id=转置卷积含义与矩阵形式>转置卷积含义与矩阵形式</h2><p>文档：
<a href=https://arxiv.org/pdf/1603.07285.pdf title="A guide to convolution arithmetic for deep learning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>A guide to convolution arithmetic for deep learning
<i class="fa fa-external-link-alt"></i>
</a>介绍了转置卷积一个概念：The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.</p><p>翻译一下就是：一个普通卷积的反向操作，已知一个卷积操作L，它输入是A，输出是B，现在需要将B重新变回具有与A形状线相同的C,并且C与A依然保持一种与卷积操作L相容（一致性）的连接特性。注意：C与B只是形状相同，但是数值一般是不同的。
另外转置卷积的作用：以一个已经编码的layer进行解码，或者将特征图映射到高维空间（or project feature maps to a higher-dimensional space.）</p><p>转置卷积也有其他的概念，而转置卷积的名称与使用矩阵运算来计算卷积有关（Convolution as a matrix operation）：</p><p>以图2.1所表示的卷积为例。如果将输入和输出从左到右、从上到下展开成向量，那么卷积可以表示为一个稀疏矩阵\(C\)，其中非零元素是卷积核的元素\(W(i,j)\)（其中 \(i\) 和 \(j\) 分别是卷积核的行和列）：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306180129432.png alt></p><center>图2.1</center><p><img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306200835487.png alt></p><center>展开的卷积核</center><p>这个线性运算将输入矩阵展平为一个16维向量\(X\)，并产生一个4维向量\(Z\)，之后被重新整形为2×2的输出矩阵。利用这种表示方法，反向传播很容易通过转置\(C\)来获取；换句话说，误差通过将损失与\(C^T\)相乘来进行反向传播。该运算以一个4维向量作为输入，并产生一个16维向量作为输出，其连接模式与\(C\)的构造方式兼容。
值得注意的是，核\(W\)定义了用于正向和反向传播的矩阵\(C\)和\(C^T\)。梯度反向传播的过程中，在计算输入\(X\)的梯度时，是通过该4维向量\(Z\)的梯度（也是反向传播过来的）与\(C^T\)相乘来计算的。</p><p>转置卷积是相对原卷积来说的。原卷积的不同卷积方式，对应的转置卷积也是不一样的。转置卷积可以通过普通的卷积来实现。</p><p>具体的实现方式参考：
<a href=https://arxiv.org/pdf/1603.07285.pdf title="A guide to convolution arithmetic for deep learning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>A guide to convolution arithmetic for deep learning
<i class="fa fa-external-link-alt"></i></a> ，在第4章有详细的介绍。</p><p>原卷积如果是No zero padding, unit strides，那么转置卷积将采用full-conv，例如：
<img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306180241711.png alt></p><center>图4.1</center><p><code>图4.1</code> 解释：
Relationship 8. A convolution described by s = 1, p = 0 and k
has an associated transposed convolution described by k′= k, s′= s
and p′= k −1 and its output size is o′= i′+ (k −1).
其中i&rsquo; = (i-k+2*p)/s + 1</p><p>值得注意：pytorch 的转置卷积默认是full-conv，pytorch 转置卷积将再第3节介绍。</p><p>另外的参考：https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html
其中有解释转置卷积的名称由来：抽象来看，给定输入向量\(\mathbf{x}\)和权重矩阵\(W\)，卷积的前向传播函数可以通过将其输入与权重矩阵相乘并输出向量\(\mathbf{y}=\mathbf{W}\mathbf{x}\)来实现。 由于反向传播遵循链式法则和\(\nabla_{\mathbf{x}}\mathbf{y}=\mathbf{W}^\top\)，卷积的反向传播函数可以通过将其输入与转置的权重矩阵\(\mathbf{W}^\top\)相乘来实现。 因此，转置卷积层能够交换卷积层的正向传播函数和反向传播函数：它的正向传播和反向传播函数将输入向量分别与\(\mathbf{W}^\top\)和\(\mathbf{W}\)相乘。</p><h2 id=转置卷积的一种简单理解>转置卷积的一种简单理解</h2><p>文档
<a href=https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html title=https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html
<i class="fa fa-external-link-alt"></i></a> 介绍了一种简单的转置卷积理解方式：
输入\(i\) 是<code>[[0,1],[2,3]]</code>, 转置卷积核也为：<code>[[0,1],[2,3]]</code>（注意：该卷积与pytorch的实现方式不一样，但这是另外一种理解，而且与pytorch的转置卷积api计算结果一致）</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306180448211.png alt></p><center>图13.10.1</center><p>该转置卷积设步幅为1且没有填充。输入张量中的每个元素都要乘以卷积核，然后产生了4个中间结果，然后加起来得到就是转置卷积的结果。
注意：图13.10.1转置卷积对应的原卷积的参数是：<code>stride = 1, padding = 0</code>。这样输出的shape : <code>i' = (i-k+2*padding)/stride + 1 == i-(k-1)</code>，而转置卷积的目的是在形状上将\(i&rsquo;\)恢复为 \(i\) ，且保持卷积的一种一致性。可以用一下代码实现，定义函数<code>trans_conv</code>，输入矩阵\(X\)和卷积核矩阵\(K\)实现基本的转置卷积运算：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trans_conv</span>(X, K):
</span></span><span style=display:flex><span>    h, w <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    Y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> h <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>, X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>+</span> w <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]):
</span></span><span style=display:flex><span>            Y[i: i <span style=color:#f92672>+</span> h, j: j <span style=color:#f92672>+</span> w] <span style=color:#f92672>+=</span> X[i, j] <span style=color:#f92672>*</span> K
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> Y
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>trans_conv(X, K)
</span></span></code></pre></div><p>输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>tensor([[ 0.,  0.,  1.],
</span></span><span style=display:flex><span>        [ 0.,  4.,  6.],
</span></span><span style=display:flex><span>        [ 4., 12.,  9.]])
</span></span></code></pre></div><p>当输入X和卷积核K都是四维张量时，我们可以使用高级API(pytorch)获得相同的结果。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>X, K <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), K<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>tconv <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>tconv<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> K
</span></span><span style=display:flex><span>tconv(X)
</span></span></code></pre></div><p>结果也是一样：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>tensor([[[[ 0.,  0.,  1.],
</span></span><span style=display:flex><span>          [ 0.,  4.,  6.],
</span></span><span style=display:flex><span>          [ 4., 12.,  9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)
</span></span></code></pre></div><p>更详细的内容，下面两个链接不错：
<a href=https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html title=转置卷积 rel="noopener external nofollow noreferrer" target=_blank class=exturl>转置卷积
<i class="fa fa-external-link-alt"></i></a>
<a href=https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8 title="Transposed Convolutions explained with… MS Excel!" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Transposed Convolutions explained with… MS Excel!
<i class="fa fa-external-link-alt"></i></a></p><h2 id=pytorch-转置卷积参数的理解及其shape的公式推导>pytorch 转置卷积参数的理解及其Shape的公式推导</h2><p>pytorch 转置卷积 的文档：
<a href="https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=convtranspose2d#torch.nn.ConvTranspose2d" title=torch.nn.ConvTranspose2d rel="noopener external nofollow noreferrer" target=_blank class=exturl>torch.nn.ConvTranspose2d
<i class="fa fa-external-link-alt"></i></a></p><p>另外下面这个文档比较重要，也是本文主要的参考文章
<a href=https://arxiv.org/pdf/1603.07285.pdf title="A guide to convolution arithmetic for deep learning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>A guide to convolution arithmetic for deep learning
<i class="fa fa-external-link-alt"></i></a></p><h3 id=pytorch-转置卷积shape的计算公式>pytorch 转置卷积shape的计算公式</h3><p>pytorch 的转置卷积包含很多参数，它的基本含义是，如果转置卷积使用原卷积相同的参数，那么转置卷积结果的shape与原卷积的输入保持一致, 这个概念很重要。
pytorch 转置卷积的函数签名：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>CLASS torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode=&#39;zeros&#39;, device=None, dtype=None)
</span></span></code></pre></div><p>pytorch转置卷积输出的shape是有计算公式的：</p><p>$$H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]\times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1 $$</p><p>$$W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
\times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1$$</p><h3 id=pytorch-转置卷积-的计算过程>pytorch 转置卷积 的计算过程</h3><p>接下来介绍该算式的含义。
为了理解转置卷积，需要结合原卷积来理解，下面一起考虑原卷积和对应的转置卷积：</p><p>假设原卷积输入特征图 \(H_{in}\)= \(i\) ，卷积核设置（kernel= \(k\) , stride= \(s\) , padding= \(p\) , dilation= \(d\) 等）输出的特征图为 \(i&rsquo;\) 。对应的转置卷积，当给一个特征图 \(i&rsquo;\), 以及给定相同的卷积核设置（stride=\(s\), padding=\(p\), dilation= \(d\) 等），在pytorch中这个设置将会输出特征图形状 \(o_t\) 与特征图 \(i\) 一致（值是不保证相同的，值通常也不相同）。</p><p>接下来我们分为2步<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>进行转置卷积操作:</p><ul><li>第一步：对输入的特征图 \(i&rsquo;\) 进行一些变换，例如：元素之间插入0，特征图周围进行padding，得到新的特征图 \(\tilde{i&rsquo;}\)</li><li>第二步：在新的特征图上做普通卷积 (kernel=\(k\), stride=1，dilation=\(d\) 等)，得到的结果 \(o_t\) 就是转置卷积的结果，就是我们要求的结果。</li></ul><h4 id=第一步对输入的特征图进行一些变换>第一步，对输入的特征图进行一些变换</h4><p>pytorch 转置卷积有一些重要的默认设置：
stride=1, padding=0, dilation=1</p><p>其中stride 控制互相关的步幅（原文是：controls the stride for the cross-correlation.）。它对应的是原卷积的stride，例如：原卷积的stride 是2，那么转置卷积的stride也设置2，如图4.6所示。但是转置卷积处理与原卷积不同：在特征图a元素中间隔插入<code>(stride-1)=1</code> 个零元素。这么做的原因是考虑一种原卷积与转置卷积具有某种<code>相同的连接模式</code>， 这个后面再解释。</p><p>转置卷积的padding 控制方式与原卷积看似比较奇怪，直接看pytorch的处理方式：不考虑dilation的情况下，dilation 后面再解释，转置卷积paddding计算公式为：<code>(kernel_size - 1) - padding</code> ，例如：转置卷积 kernel_size=3，padding=1, 那么实际的padding为 <code>(3-1)-1 = 1</code>, 注意这里的padding 指的是单边padding的大小。如果考虑两边要乘2。
这里有个规律<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> : 当原卷积padding=0时，转置卷积: <code>padding=(kernel_size -1)</code>, 这种padding表明转置卷积是一种全卷积(full-convolution), 也称为：full padding。当原卷积是全卷积时，考虑上面转置卷积paddding计算公式，那么转置卷积的padding=0，即: <code>padding=(kernel_size-1) - (kernel_size-1)=0</code> 。当然还有<code>half-padding</code>: 原卷积与转置卷积此时有相同的padding: \(\lfloor (k -1)/2 \rfloor\)。</p><p>转置卷积 padding 的控制方式同样要考虑 dilation 的情况 (dilation 参考 <sup id=fnref1:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> 中 5.1 Dilated convolutions)。转置卷积paddding公式为：<code>dilation * (kernel_size - 1) - padding</code> ，例如：转置卷积 kernel_size=3，padding=1 , dilation=1, 那么实际的padding为 <code>2*(3-1)-1 = 3</code>。直观理解dilation（空洞卷积）就是某种程度上<code>卷积核变大了</code>，那么对应的padding也要变大。才能保证原卷积与转置卷积具有某种<code>相同的连接模式</code>。
注意：<code>dilation * (kernel_size - 1) - padding</code>， 这里padding 是指原卷积的padding, 也就是说pytorch 中输入的参数：strides， padding 都是指原卷积的padding。</p><p>第一步：对输入的特征图 \(i&rsquo;\) 进行一些变换得到\(\tilde{i&rsquo;}\) 就介绍到这里，虽然不止这些，但对理解pytorch的转置卷积输出shape的计算公式差不多够了。由参数strides和padding变换得到\(\tilde{i&rsquo;}\)大小为：$$H_{\tilde{i&rsquo;}}= i&rsquo; + (i&rsquo;-1) * (s-1) + 2[d*(k-1)-p]$$</p><p>转置卷积、padding 之所以这样处理是有原因的<sup id=fnref2:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>，原卷积与转置卷积具有某种<code>相同的连接模式</code>的直观理解：
One way to understand the logic behind zero padding is to consider the connectivity pattern of the transposed convolution and use it to guide the design of the equivalent convolution. For example, the top left pixel of the input of the direct convolution only contribute to the top left pixel of the output, the top right pixel is only connected to the top right output pixel, and so on.<br>To maintain the same connectivity pattern in the equivalent convolution it is necessary to zero pad the input in such a way that the first (top-left) application of the kernel only touches the top-left pixel, i.e., the padding has to be equal to the size of the kernel minus one.
翻译一下就是：理解零填充背后的逻辑的一种方法是考虑反卷积的连接模式，并使用它来指导等价的卷积（转置卷积实际的实现方式）设计。例如，直接卷积输入的左上角像素仅对输出的左上角像素有贡献，右上角像素仅连接到右上角输出像素，依此类推。以图4.1为例，为了在等价卷积（转置卷积实际的实现方式）中保持相同的连接模式，需要以零填充输入，使得核的第一次（左上角）卷积仅接触左上角像素，即填充必须等于核大小减一（仅仅是图4.1的情况，）。
例如：
<img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306182308871.png alt>
图4.1<sup id=fnref3:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>是一个转置卷积的例子，原卷积是：<code>3*3 kernel over a 4*4 input using unit strides(i.e., i = 4, k=3, s=1 and p=0)</code>, 转置卷积的等价卷积是：<code>3*3 kernel over a 2*2 input , padding=2, strides = 1. (i.e., i'=2, k'=k=3, s'=1, and p'= 2 = (k-1) - p</code></p><p>总的来说，输入的特征图 \(i&rsquo;\) 进行一些变换之所以要这样处理(stride 、padding等)是为了保持同样的连接性：这是指从A到B（AB分别表示卷积前和卷积后的特征图），如果A中一个位置与B中一个位置通过kernel有关系，那么在卷积核逆卷积中有相同的连通。<sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>另外一个例子：
<img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306181735964.png alt>图4.6<sup id=fnref4:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><h4 id=第二步对变换得到特征图进行普通的卷积>第二步，对变换得到特征图进行普通的卷积</h4><p>第二步思路比较简单，对变换得到\(\tilde{i&rsquo;}\) 进行普通的卷积，kernel = k, strides=1, padding=0， dilation=d, 由于考虑dialation, 等效的卷积核（感受野）大小为：\(k+(k-1)*(d-1)\)</p><p>由第一步可知：由参数strides和padding变换得到特征图\(\tilde{i&rsquo;}\)大小为：
$$H_{\tilde{i&rsquo;}}= i&rsquo; + (i&rsquo;-1) * (s-1) + 2[d*(k-1)-p]$$</p><p>转置卷积shape计算：\(o_t=\frac{H_{i&rsquo;} - [k+(k-1)*(d-1)]}{1} + 1\)</p><p>公式化简一下就是：\(o_t=(i&rsquo;-1)s - 2p + d(k-1) + 1\)</p><p>但是pytorch 的计算公式是：
$$H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]\times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1 $$
对比一下发现少了一项：\(\text{output_padding}[0]\)
\(\text{output_padding}[0]\) 这一项是为了解决转置卷积同一个 \(i&rsquo;\) 对应多个 \(i\) 的问题，即原卷积的输出\(i&rsquo;\) 可能因为不同的strides、padding设置而对应多个不同shape的 \(i\) , 这时候需要添加额外的参数\(\text{output_padding}[0]\)使得转置卷积还原为一个特定的 \(i\) 的形状，它的操作是单边补零的，pytorch 参数解释：Additional size added to one side of each dimension in the output shape. Default: 0。</p><p>例如：
<img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306190351612.png alt></p><p>于是加上\(\text{output_padding}\)的转置卷积shape计算公式：</p><p>\(o_t=(i&rsquo;-1)*s -2p + d(k-1) +\text{output_padding} +1\) 。它与pytorch 的给出的公式一致。</p><p>参考：
<a href=https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html title=转置卷积 rel="noopener external nofollow noreferrer" target=_blank class=exturl>转置卷积
<i class="fa fa-external-link-alt"></i></a>
<a href=https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8 title="Transposed Convolutions explained with… MS Excel!" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Transposed Convolutions explained with… MS Excel!
<i class="fa fa-external-link-alt"></i></a></p><h2 id=卷积与数学上的卷积>卷积与数学上的卷积</h2><h3 id=pytorch-的转置卷积的核旋转了180度>pytorch 的转置卷积的核旋转了180度</h3><p>将第一节（转置卷积的直观理解）与第二节（pytorch 转置卷积参数的理解，以及转置卷积Shape的公式推导）进行对比，你会发现这里两种操作完全不同，但是得到了相同的结果。这其中必然有某种联系。
还是考虑第一节的例子：
<img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306180448211.png alt>
图13.10.1
采用pytorch 的做法，第一步先将输入\(i&rsquo;\) （转置卷积的输入，另外\(i\)表示的是原卷积的输入）进行变换，假设原卷积的参数是：kernel = 2, strides = 1，padding=0。
strides =1, 输入\(i&rsquo;\) 元素之间不需要插入0
padding=0, 实际的padding = (k-1)-p = (2-1)-0 = 1
那么变换后的 \(\tilde{i&rsquo;}\) （转置卷积的输出）：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://qyzhizi.cn/img/202306201141726.png alt>
然后进行第二步：对变换后的 \(\tilde{i&rsquo;}\) 进行普通的卷积：k=2, s=1, p=0
k =<code>[[0,1],[2,3]]</code>
看起来不复杂，可以直接手算，得到卷积的结果：\(o_1\) = <code>[[0,3,2],[6,14, 6],[2,3,0]]</code>，这与图13.10.1的输出完全不同。
但是你如果用pytorch的转置卷积api 算一下，又会发现结果与13.10.1的输出完全一致。例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>X, K <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), K<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>tconv <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ConvTranspose2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>tconv<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> K
</span></span><span style=display:flex><span>tconv(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>输出<span style=color:#960050;background-color:#1e0010>：</span>
</span></span><span style=display:flex><span>tensor([[[[ <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>4.</span>,  <span style=color:#ae81ff>6.</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>4.</span>, <span style=color:#ae81ff>12.</span>,  <span style=color:#ae81ff>9.</span>]]]], grad_fn<span style=color:#f92672>=&lt;</span>ConvolutionBackward0<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><p>那么是哪里出了问题呢？
问题在第二步，对变换后的 \(\tilde{i&rsquo;}\) 进行普通的卷积时，卷积核需要旋转180度。新的卷积核是：
k_new = <code>[[3.0, 2.0], [1.0, 0.0]]</code>, 对变换后的 \(\tilde{i&rsquo;}\) 进行普通的卷积（k=2, s=1, p=0）。结果是：\(o_2\) = <code>[[0,0,1],[0,4, 6],[4,12,9]]</code> 这就与pytorch的计算一致了。
这里用pytorch Conv2d (普通卷积api) 进行了验证：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0.0</span>, <span style=color:#ae81ff>1.0</span>], [<span style=color:#ae81ff>2.0</span>, <span style=color:#ae81ff>3.0</span>]])
</span></span><span style=display:flex><span>T_K <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>2.0</span>], [<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>0.0</span>]])
</span></span><span style=display:flex><span>X, K, T_K <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), K<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>), T_K<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>nn_conv2d</span>(im, kernel):
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> <span style=color:#75715e># 用nn.Conv2d定义卷积操作</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> conv_op <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> <span style=color:#75715e># 给卷积操作的卷积核赋值</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> conv_op<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> kernel
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> <span style=color:#75715e># 对图像进行卷积操作</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> conv_res <span style=color:#f92672>=</span> conv_op(im)
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010> </span> <span style=color:#960050;background-color:#1e0010> </span> <span style=color:#66d9ef>return</span> conv_res
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> nn_conv2d(X, K)
</span></span><span style=display:flex><span>print(res)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>res2 <span style=color:#f92672>=</span> nn_conv2d(X, T_K)
</span></span><span style=display:flex><span>print(res2)
</span></span></code></pre></div><p>结果是：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>tensor([[[[ <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>3.</span>,  <span style=color:#ae81ff>2.</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>6.</span>, <span style=color:#ae81ff>14.</span>,  <span style=color:#ae81ff>6.</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>2.</span>,  <span style=color:#ae81ff>3.</span>,  <span style=color:#ae81ff>0.</span>]]]], grad_fn<span style=color:#f92672>=&lt;</span>ConvolutionBackward0<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>tensor([[[[ <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>0.</span>,  <span style=color:#ae81ff>4.</span>,  <span style=color:#ae81ff>6.</span>],
</span></span><span style=display:flex><span>          [ <span style=color:#ae81ff>4.</span>, <span style=color:#ae81ff>12.</span>,  <span style=color:#ae81ff>9.</span>]]]], grad_fn<span style=color:#f92672>=&lt;</span>ConvolutionBackward0<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><p>可以看到<code>nn_conv2d(X, T_K)</code>函数输出了正确结果，这说明nn.Conv2d 没有对卷积核进行180度旋转，但是转置卷积<code>nn.ConvTranspose2d</code>对卷积核进行180度旋转。</p><p>旋转卷积核后与第一节中的计算结果保持一致，第一节中的计算简单描述就是：每个元素直接与卷积核相乘，然后将所有的中间结果进行叠加。当然这种操作还涉及到strides大于1的情况<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>,这里就不解释了。</p><h3 id=数学上定义的卷积>数学上定义的卷积</h3><p>其实原本数学上的卷积就是要将卷积核进行旋转180度，数学上的卷积与<code>图13.10.1</code> 的操作应该存在着联系。即：<code>图13.10.1</code> 的操作 与使用翻转180度的<code>[[0,1],[2,3]]</code>对 \(\tilde{i&rsquo;}\) 进行卷积是等价的。
这种联系不打算深究。接下来本文打算给出一些直观的例子。</p><p>在此之前先解释一下数学上的卷积是怎么回事。</p><ul><li><p>在数学上，连续形式的卷积定义如下：</p></li><li><p>设 \(f(x)\) 和 \(g(x)\) 是在实数域上的两个可积函数，定义它们的卷积 \(h(x)\) 为：</p></li><li><p>\(h(x) = (f*g)(x) = \int_{-\infty}^{\infty} f(\tau)g(x-\tau) d\tau\)</p></li><li><p>其中，\(*\) 表示卷积操作，\(h(x)\) 表示卷积的输出，\(f(x)\) 和 \(g(x)\) 分别表示卷积的输入函数，\(\tau\) 是积分变量。</p></li><li><p>卷积的计算过程可以理解为将 \(f(x)\) 和 \(g(x)\) 进行平移、翻转、相乘和积分的过程。具体来说，对于 \(g(\tau)\)，首先将其进行翻转得到 \(g(-\tau)\)，然后将其在 \(x\) 轴上平移 \(x\) 个单位得到 \(g(x-\tau)\)，然后再与 \(f(\tau)\) 相乘，并对 \(\tau\) 进行积分，最终得到卷积的输出 \(h(x)\)。</p></li><li><p>图像的卷积是离散形式，离散形式的卷积定义如下：</p></li><li><p>设 \(f[n]\) 和 \(g[n]\) 是在整数域上的两个离散序列，定义它们的卷积 \(h[n]\) 为：</p></li><li><p>\(h[n] = (f*g)[n] = \sum_{m=-\infty}^{\infty} f[m]g[n-m]\)</p></li><li><p>离散形式与连续形式类似，当确定一个n后，计算 \(\sum_{m=-\infty}^{\infty} f[m]g[n-m]\) 时，不是积分而是求和。如果与图像的卷积类别。每个n 对应卷积核的一次平移，计算\(\sum_{m=-\infty}^{\infty} f[m]g[n-m]\)对应一次将卷积核旋转180度然后对应元素直接进行相乘求和。</p></li><li></li></ul><p>卷积可以用概括为：翻转、平移、相乘再求和
先对g函数进行翻转，相当于在数轴上把g函数从右边褶到左边去，也就是卷积的“卷”的由来。
然后再把g函数平移到n，在这个位置对两个函数的对应点相乘，然后相加，这个过程是卷积的“积”的过程。<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>
例如：
<img src=/imgs/img-lazy-loading.gif data-src="https://picx.zhimg.com/80/v2-847a8d7c444508862868fa27f2b4c129_1440w.webp?source=1940ef5c" alt=信号分析>
更具体的例子可看
<a href=https://www.zhihu.com/question/22298352/answer/637156871 title="如何通俗易懂地解释卷积？ - palet的回答 - 知乎" rel="noopener external nofollow noreferrer" target=_blank class=exturl>如何通俗易懂地解释卷积？ - palet的回答 - 知乎
<i class="fa fa-external-link-alt"></i>
</a>中的<code>信号分析</code>、<code>丢骰子</code>与<code>图像处理</code>的例子。</p><h3 id=多项式系数卷积与意义>多项式系数卷积与意义</h3><p>接下来是一个一维矩阵的卷积，希望有更直观数学意义<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>，这里只是给出一种直观的感觉，不解释这么做的原因，因为笔者也不太清楚。</p><p>考虑两个多项式函数：
\(y=3x+2\)
\(y=2x^2+3x-1\)
将这两个函数相乘：\(y=(3x+2)(2x^2+3x-1)= 6x^2+13x+3x-2\)
可以将这两个函数的系数按x的阶数从大到小考虑为两个矩阵，然后进行卷积：
\(y=3x+2\) 的系数矩阵：i=<code>[3,2]</code>
\(y=2x^2+3x-1\) 的系数矩阵：K=<code>[2, 3, -1]</code>，当成卷积核。旋转180度后卷积核：T_K=<code>[-1, 3, 2]</code></p><p>将两个矩阵进行卷积：</p><ul><li>首先将系数矩阵<code>[3,2]</code> 进行padding: <code>[0,0,3,2,0,0]</code>, padding= 2 = kernel_size -1 = 3 - 1。</li><li>移动卷积核：T_K=<code>[-1, 3, 2]</code>进行计算：</li><li>例如：</li></ul><table><thead><tr><th>0</th><th>0</th><th>3</th><th>2</th><th>0</th><th>0</th></tr></thead><tbody><tr><td>-1</td><td>3</td><td>2</td><td></td><td></td><td></td></tr></tbody></table><table><thead><tr><th>0</th><th>0</th><th>3</th><th>2</th><th>0</th><th>0</th></tr></thead><tbody><tr><td></td><td>-1</td><td>3</td><td>2</td><td></td><td></td></tr></tbody></table><table><thead><tr><th>0</th><th>0</th><th>3</th><th>2</th><th>0</th><th>0</th></tr></thead><tbody><tr><td></td><td></td><td>-1</td><td>3</td><td>2</td><td></td></tr></tbody></table><table><thead><tr><th>0</th><th>0</th><th>3</th><th>2</th><th>0</th><th>0</th></tr></thead><tbody><tr><td></td><td></td><td></td><td>-1</td><td>3</td><td>2</td></tr></tbody></table><p>最后结果是：<code>[2*3, 3*3+2*2, 2*3-1*3, -1*2]</code> = <code>[6, 13, 3, 2]</code> 恰好是\(6x^2+13x+3x-2\) 的系数。感觉翻转180度进行卷积与多项式的计算有关联，计算的结果是有意义的，这些系数是\(x^n\)的系数，或者说是某种特征的系数。</p><p>另外考虑另外一种实现方式，类似于图13.10.1，也类似于\(y=(3x+2)(2x^2+3x-1)= 6x^2+13x+3x-2\)的化简过程：
将矩阵：i=<code>[3,2]</code>的每个元素与系数矩阵：K=<code>[2, 3, -1]</code>相乘，例如：
<code>[3]*[2,3,-1]</code> -> <code>[6, 9, -3]</code>
<code>[2]*[2,3,-1]</code> -> <code>[4, 6, -2]</code></p><p>然后将这两个中间结果矩阵按下面的形式相加，最后也得到了同样的系数。</p><table><thead><tr><th>6</th><th>9</th><th>-3</th><th></th></tr></thead><tbody><tr><td></td><td>4</td><td>6</td><td>-2</td></tr><tr><td>6</td><td>13</td><td>3</td><td>-2</td></tr></tbody></table><p>思考：这里的中间结果相加对齐了两个元素（考虑\(x^n\)的合并），而<code>图13.10.1</code> 中间结果只是对齐了一个元素，这是为什么？可能与卷积核的大小有关，如果卷积核是<code>4x4</code> 那么中间结果相加是不是要对齐3个元素？</p><p>参考：
<a href=https://www.zhihu.com/question/22298352/answer/637156871 title="如何通俗易懂地解释卷积？ - palet的回答 - 知乎" rel="noopener external nofollow noreferrer" target=_blank class=exturl>如何通俗易懂地解释卷积？ - palet的回答 - 知乎
<i class="fa fa-external-link-alt"></i></a></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://blog.csdn.net/qq_27261889/article/details/86304061 title=https://blog.csdn.net/qq_27261889/article/details/86304061 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://blog.csdn.net/qq_27261889/article/details/86304061
<i class="fa fa-external-link-alt"></i>
</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://arxiv.org/pdf/1603.07285.pdf title="A guide to convolution arithmetic for deep learning" rel="noopener external nofollow noreferrer" target=_blank class=exturl>A guide to convolution arithmetic for deep learning
<i class="fa fa-external-link-alt"></i>
</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref3:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref4:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8 title="Transposed Convolutions explained with… MS Excel!" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Transposed Convolutions explained with… MS Excel!
<i class="fa fa-external-link-alt"></i>
</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><a href=https://www.zhihu.com/question/22298352/answer/637156871 title="如何通俗易懂地解释卷积？ - palet的回答 - 知乎" rel="noopener external nofollow noreferrer" target=_blank class=exturl>如何通俗易懂地解释卷积？ - palet的回答 - 知乎
<i class="fa fa-external-link-alt"></i>
</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><a href=http://t.csdn.cn/SkaKe title="理解卷积的数学意义 卷积分" rel="noopener external nofollow noreferrer" target=_blank class=exturl>理解卷积的数学意义 卷积分
<i class="fa fa-external-link-alt"></i>
</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><div class=post-tags><a href=/tags/%e8%bd%ac%e7%bd%ae%e5%8d%b7%e7%a7%af>转置卷积</a>
<a href=/tags/%e5%8d%b7%e7%a7%af>卷积</a>
<a href=/tags/programming>programming</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=reward-container><div><i class="fa-solid fa-mug-hot"></i>请我喝杯咖啡吧 ヾ(^▽^*)))</div><button>赞赏</button><div class=post-reward></div></div><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
转置卷积与卷积的理解</li><li class=post-copyright-author><strong>原文作者：</strong>
Hollis</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/trans-conv-and-conv.html title=转置卷积与卷积的理解>/post/trans-conv-and-conv.html</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/rss.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"></div><div class="post-nav-prev post-nav-item"><a href=/handwriting-dl-framework.html rel=prev title="Handwriting Dl Framework">Handwriting Dl Framework
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div id=gtranslate class=google-translate><i class="fa fa-language"></i><div id=google_translate_element></div></div><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Hollis</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":true,"save":"manual"},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":"https://walinejs.comment.lithub.cc","sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.37028fbafbd97fd89808b4c7b5a3a81f01ed0ab24001d273d774f9546a0e9170.js defer></script>
<script type=text/javascript src=/js/math.min.a6ada19a368d85dad9ead2040d86ae561a867fafef89391d1aa2aa5909366509.js defer></script></body></html>